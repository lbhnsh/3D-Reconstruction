---
attachments: [Clipboard_2023-09-26-18-12-54.png, Clipboard_2023-09-26-18-13-30.png, Clipboard_2023-09-26-18-14-40.png, Clipboard_2023-09-26-18-43-55.png, Clipboard_2023-09-26-18-49-13.png, Clipboard_2023-09-26-18-51-17.png, Clipboard_2023-09-26-19-04-54.png, Clipboard_2023-09-26-19-13-52.png, Clipboard_2023-09-26-19-20-48.png, Clipboard_2023-09-26-19-29-19.png, Clipboard_2023-09-26-19-32-43.png, Clipboard_2023-09-26-19-51-20.png]
title: C4_W2
created: '2023-09-26T12:26:23.273Z'
modified: '2023-09-29T04:16:14.306Z'
---

# C4_W2

__Classical Networks__

LeNet - 5

![](@attachment/Clipboard_2023-09-26-18-12-54.png)

AlexNet

![](@attachment/Clipboard_2023-09-26-18-13-30.png)

VGG - 16

![](@attachment/Clipboard_2023-09-26-18-14-40.png)


__Residual Networks__

Problem in very deep neural network :
Very deep neural networks can represent very complex functions but there is problem of vanishing gradients. Very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow. More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step. If the gradients are small, due to large number of multiplications, the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and â€œexplodeâ€ to take very large values).
The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say  ð‘Ž[ð‘™]
 ) has the same dimension as the output activation (say  ð‘Ž[ð‘™+2])

VGG-16 is a 'plain' network. In plain networks, as the number of layers increase from 20 to 56 (as shown below), even after thousands of iterations, training error was worse for a 56 layer compared to a 20 layer network.
![](@attachment/Clipboard_2023-09-26-18-43-55.png)

When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly.

Using deeper networks is degrading the performance of the model. Microsoft Research paper tries to solve this problem using __Deep Residual learning framework__.

The idea is that instead of letting layers learn the underlying mapping, let the network fit the residual mapping. So, instead of say H(x), initial mapping, let the network fit, F(x) := H(x)-x which gives H(x) := F(x) + x

__The approach is to add a shortcut or a skip connection that allows information to flow, well just say, more easily from one layer to the nextâ€™s next layer, i.e., you bypass data along with normal CNN flow from one layer to the next layer after the immediate next.__

A residual block looks like
![](@attachment/Clipboard_2023-09-26-18-49-13.png)

Model trained using residual blocks
![](@attachment/Clipboard_2023-09-26-18-51-17.png)

__1 X 1 convolution__

[6 X 6] * [2] = [6 X 6 X 1]
[6 X 6 X 32] * [1 X 1 X 32] = [6 X 6 X no of filters]

A 28 X 28 X 192 block gives 28 X 28 X 192 block on applying 1 X 1 convolution using ReLU activation.

__Motivation for Inception network__

[28 X 28 X 192] ---> 1 X 1 ---> [28 X 28 X 64]

[28 X 28 X 192] ---> 3 X 3 ---> [28 X 28 X 128]

[28 X 28 X 192] ---> 5 X 5 ---> [28 X 28 X 32]

[28 X 28 X 192] ---> MAX-POOL ---> [28 X 28 X 32]

To make all dimensions match, we need to add padding to max pooling layer called inception layer.
Problem with inception layer is computational cost

![](@attachment/Clipboard_2023-09-26-19-04-54.png)
Let's look at its computational cost
32 filters therefore 5 X 5 X 192
28 X 28 X 32  *  5 X 5 X 192  =  120M
creates a bottleneck situation for flow of functions through layers

__Inception Module__
It takes as input the activation or output from previous layers

28X28X192 ---> 1X1Conv ---> 5X5Conv --->28X28X32
![](@attachment/Clipboard_2023-09-26-19-13-52.png)
Concatenate output through different dimension channels 

__MobileNet__

Motivation : 
1. Low computational cost at deployment
2. Useful for mobile and embedded vision applications
3. Key idea : Normal vs Depthwise separable convolutions

Normal Convolution 
![](@attachment/Clipboard_2023-09-26-19-20-48.png)

Depthwise Separable Convolution

2 filters (depthwise convolution and pointwise convolution) 
one filter layer is applied to one matrix layer (channel) at a time

![](@attachment/Clipboard_2023-09-26-19-29-19.png)

Computational cost = #filter params X #filter positions X #of filters
432 = 3X3  X  4X4  X  3

The next step is to take this 4X4X3 set of values and apply pointwise convolution

![](@attachment/Clipboard_2023-09-26-19-32-43.png)

Computational cost = #filter params X #filter positions X #of filters
240 = 1X1X3  X  4X4  X  5

When calling image_data_set_from_directory(), specify the train/val subsets and match the seeds to prevent overlap
Use prefetch() to prevent memory bottlenecks when reading from disk
Give your model more to learn from with simple data augmentations like rotation and flipping.
When using a pretrained model, it's best to reuse the weights it was trained on.

MobileNetV2's unique features are:
Depthwise separable convolutions that provide lightweight feature filtering and creation
Input and output bottlenecks that preserve important information on either end of the block
Depthwise separable convolutions deal with both spatial and depth (number of channels) dimensions



Summary -
cost of normal convolution 2160
cost of depthwise separable convolution
= depthwise + pointwise
= 432 + 240
= 672

Depthwise separable convolution works for any number of input channels

__MobileNet v1 architecture__

raw image ---> 13 Depthwise separable convolution layers ---> pooling layer ---> Fully connected layer ---> Softmax layer

but becomes computationally expensive

__MobileNet v2 architecture__
2 main changes-
addition of a residual connection (facilitates efficient backward propagation)
(acts as expansion layer)

working of residual block
![](@attachment/Clipboard_2023-09-26-19-51-20.png)

With help of residual block, we can compute richer and more complex set of functions while keeping the size of memory i.e. no of activation functions to be passed from layer to layer relatively small

__Data Augmentation__
Mirroring
Random Cropping
Rotation
Shearing
Local warping

Color shifting
using PCA Color Augmentation

__Data vs Hand engineering__

Little data (more hand engineering) <----------------------> Lots of data (less hand engineering)
required data size (small to large)
Object detection
Image recognition
Speech recognition

Two sources of knowledge 
1. labeled data
2. Hand engineered features

For doing well on benchmarks :-
Train several networks independently and average their outputs
Run classifier on multiple versions of test images and average results

















































