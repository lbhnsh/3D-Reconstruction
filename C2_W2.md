---
attachments: [Clipboard_2023-09-13-18-30-08.png]
title: C2_W2
created: '2023-09-13T10:04:08.880Z'
modified: '2023-09-13T13:21:10.213Z'
---

# C2_W2
__Mini-batch gradient descent__

It is an iterative optimization algorithm used to find the minimum of a cost function. Unlike Batch Gradient Descent that processes the entire training set at once, mini-batch Gradient Descent processes small random subsets of the training data (mini-batches) at each iteration. This reduces the computational cost and speeds up the convergence of the algorithm. Moreover it can also take advantage of vectorization, which can speed up the computation of the gradient.

If small traiinig set (m<2000) : Use batch gradient descent else mini-batch gradient
Typical mini-batch sizes :
64, 128, 256, 512

___

__Exponentially weighed averages__

V<sub>t</sub> = β * (V<sub>t</sub> - 1) + (1-β) * θ<sub>t</sub>

By choosing the β value, we control how much weight to give for the last N data values in calculating the average.

Implementation

V<sub>0</sub> = 0
V<sub>1</sub> = β * (V<sub>0</sub>) + (1-β) * θ<sub>1</sub>
V<sub>2</sub> = β * (V<sub>1</sub>) + (1-β) * θ<sub>2</sub>
V<sub>3</sub> = β * (V<sub>2</sub>) + (1-β) * θ<sub>3</sub>

___

__Gradient descent with momentum__

The problem with gradient descent is that the weight update at a moment (t) is governed by the learning rate and gradient at that moment only. It doesn’t take into account the past steps taken while traversing the cost space.
It leads to some problems -
1. The gradient of the cost function at saddle points( plateau) is negligible or zero, which in turn leads to small or no weight updates. Hence, the network becomes stagnant, and learning stops
2. The path followed by Gradient Descent is very jittery even when operating with mini-batch mode

cost surface for example
![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*aw3Wm0KDe_MDwApwNTeqZA.jpeg)
Let’s assume the initial weights of the network under consideration correspond to point A. With gradient descent, the Loss function decreases rapidly along the slope AB as the gradient along this slope is high. But as soon as it reaches point B the gradient becomes very low. 
Ideally, cost should have moved to the global minima point C, but because the gradient disappears at point B, we are stuck with a sub-optimal solution.

How can momentum fix this ?
 Imagine you have a ball rolling from point A. The ball starts rolling down slowly and gathers some momentum across the slope AB. When the ball reaches point B, it has accumulated enough momentum to push itself across the plateau region B and finally following slope BC to land at the global minima C. To account for the momentum, we can use a moving average over the past gradients. In regions where the gradient is high like AB, weight updates will be large. Thus, in a way we are gathering momentum by taking a moving average over these gradients. 

Implementation:

On iteration t:
Compute dW and db on the current mini-batch
V<sub>dW</sub> = β * V<sub>dW</sub> + dW
V<sub>db</sub> = β * V<sub>db</sub> + db

W = W - α*V<sub>dW</sub>
b = b - α*V<sub>db</sub>

___

__RMS (Root mean square) prop__

It uses the same concept of the exponentially weighted average of gradient as gradient descent with momentum but the difference is parameter update. It accelerate gradient descent.

![](https://miro.medium.com/v2/resize:fit:828/format:webp/0*V6bRtFXFaDl9LXE3.jpg)

We start gradient descent from point ‘A’ and we through end up at point ‘B’ after one iteration of gradient descent. Then another phase of downward gradient can end at ‘C’ level. With through iteration of gradient descent, with oscillations up and down, we step towards the local optima. If we use higher learning rate then the frequency of the vertical oscillation would be greater.This vertical oscillation therefore slows our gradient descent and prevents us from using a much higher learning rate. 
The ‘bias’ is responsible for the vertical oscillations whereas the movement in the horizontal direction is defined by the weight. If we slow down the update for ‘bias’ then the vertical oscillations can be dampened and if we update ‘weights’ with higher values then we can move quickly towards the local optima still.

Implementation:
We use dW and db to update our parameters W and b during the backward propagation as follows:
W = W - α*V<sub>dW</sub>
b = b - α*V<sub>db</sub>

In RMSprop we take the exponentially weighted averages of the squares of dW and db instead of using dW and db separately for each epoch.

SdW = β * SdW + (1 — β) * dW2
Sdb = β * Sdb + (1 — β) * db2

Where beta ‘β’ is a different hyperparameter called momentum, ranging from 0 to 1. To calculate the new weighted average, it sets the weight between the average of previous values and the current value.

We’ll update our parameters after calculating the exponentially weighted averages.

W = W — learning rate *dW / sqrt(SdW)

b = b — learning rate * db / sqrt(Sdb)

SdW is relatively small so here we divide dW by relatively small number while Sdb is relatively large so we divide db with a comparatively larger number to slow down the changes on a vertical dimension.

The momentum (beta) must be higher (0.80-0.99) to smooth out the update because we give more weight to the past gradients.
___

__Adam algorithm__

The Adam algorithm calculates an exponential weighted moving average of the gradient and then squares the calculated gradient. This algorithm has two decay parameters that control the decay rates of these calculated moving averages.

V<sub>dW</sub> = 0, S<sub>dW</sub> = 0
V<sub>db</sub> = 0, S<sub>dW</sub> = 0
On iteration t,
  compute dW, db using current mini - batch
  V<sub>dW</sub> = β<sub>1</sub> * V<sub>dW</sub> + (1-β<sub>1</sub>) * dW
  V<sub>db</sub> = β<sub>1</sub> * V<sub>db</sub> + (1-β<sub>1</sub>) * db
  S<sub>dW</sub> = β<sub>2</sub> * S<sub>dW</sub> + (1-β<sub>2</sub>) * dW
  S<sub>dW</sub> = β<sub>2</sub> * A<sub>dW</sub> + (1-β<sub>2</sub>) * db

  corrected values:
  V<sub>dW</sub> = V<sub>dW</sub> / (1-β<sub>1</sub>)
  V<sub>db</sub> = V<sub>db</sub> / (1-β<sub>1</sub>)
  S<sub>dW</sub> = S<sub>dW</sub> / (1-β<sub>2</sub>)
  S<sub>db</sub> = S<sub>db</sub> / (1-β<sub>2</sub>)
  ![](https://1drv.ms/i/s!Ao0tbw-VcjB7hoQ1P2VwSbz4fGx5pA?e=W33zFN)
  ![](@attachment/Clipboard_2023-09-13-18-30-08.png)

___

__Learning Rate Decay__
The idea behind learning rate decay is to gradually reduce the learning rate over time as the model gets closer to the optimal solution. The intuition is that a high learning rate may lead to overshooting the minimum, whereas a low learning rate may result in slow convergence. By gradually reducing the learning rate, we can balance these two factors and achieve faster convergence to the optimal solution. The most common way of reducing the learning rate is to multiply α by a decay factor δ after a fixed number of epochs or iterations. The new learning rate α_new is given by:

α_new = α * δ              
where 0 < δ < 1


Other way (using epoch)
1 epoch = 1 pass through dot
α = (1 / (1 + decay_rate * epoch_num)) * α<sub>0</sub>

|epoch|α|
|-----|-|
|1|0.1|
|2|0.67|
|3|0.5|
|4|0.4|
|.|.|
|.|.|
|.|.|


Learning rate decay can be applied to a wide range of optimization algorithms such as Stochastic Gradient Descent, Adam, and RMSProp. 































