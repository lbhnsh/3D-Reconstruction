---
attachments: [Clipboard_2023-09-30-00-11-23.png, Clipboard_2023-09-30-10-05-59.png, Clipboard_2023-09-30-10-37-32.png, Clipboard_2023-09-30-10-38-51.png, Clipboard_2023-09-30-10-43-24.png, Clipboard_2023-09-30-12-03-40.png, Clipboard_2023-09-30-13-43-55.png, Clipboard_2023-09-30-13-54-26.png, Clipboard_2023-09-30-14-13-24.png, Clipboard_2023-09-30-14-14-34.png, Clipboard_2023-09-30-14-15-11.png, Clipboard_2023-09-30-14-15-53.png, Clipboard_2023-09-30-14-36-23.png, Clipboard_2023-09-30-14-37-02.png]
title: C4_W4
created: '2023-09-29T18:24:08.737Z'
modified: '2023-09-30T09:07:02.890Z'
---

# C4_W4

Major challenge in face recognition -
One shot learning. Giving output from one single image i.e. person's face. 

Learning a __similarity__ function
d(img1, img2) = degree of difference between images
if d(img1, img2) <= ùùâ  -------------"same"
if d(img1, img2) > ùùâ   ----------"different"

__Siamese networks__

![](@attachment/Clipboard_2023-09-30-00-11-23.png)

What we want to do really is train the neural network so that the encoding that it computes results in a function d that tells you when two pictures are of the same person. So the parameters of the neural network define an encoding f of xi. So given any input image xi, the neural network outputs this 128 dimensional encoding f of xi.  If two pictures, xi and xj, are of the same person, then we want that distance between their encodings to be small. In contrast, if xi and xj are of different persons, then you want that distance between their encodings to be large. We can use backpropagation to vary all those parameters in order to make sure these conditions are satisfied. 

How to define objective function ?
One way to learn the parameters of the neural network, so that it gives you a good encoding for your pictures of faces, is to define and apply gradient descent on the triplet loss function. To apply the triplet loss we need to compare pairs of images. For pair of images of the same person we want their encodings to be similar, whereas for different persons encodings need to be quite different. In terms of triplet loss, always look at one anchor image and then find distance between anchor, a positive image and a negative image.

||f(A) - f(P)||<sup>2</sup> <= ||f(A) - f(N)||<sup>2</sup>

Improvement using margin parameter
||f(A) - f(P)||<sup>2</sup>+Œ± <= ||f(A) - f(N)||<sup>2</sup>

Loss function
L(A, P, N) = max(||f(A) - f(P)||<sup>2</sup> - ||f(A) - f(N)|| + Œ±, 0)

J = <sup>m</sup> Œ£ <sub>i=1</sub> L(A<sup>(i)</sup>, P<sup>(i)</sup>, N<sup>(i)</sup>)

During training, if A, P, N are chosen randomly, d(A,P) + Œ± <= d(A,N)
is easily satisfied

__Similarity function__

![](@attachment/Clipboard_2023-09-30-10-05-59.png)

y<sub>hat</sub> is 0 if images are different and 1 if images are same

__Neural Style Transfer__
Neural Style Transfer allows you to do is generated new image like the one below
![](@attachment/Clipboard_2023-09-30-10-37-32.png)
![](@attachment/Clipboard_2023-09-30-10-38-51.png)

__Visualizing what are deep ConvNets learning ?__

![](@attachment/Clipboard_2023-09-30-10-43-24.png)

Pick a unit in layer 1. Find the nine image patches that maximize the unit's activation
Repeat for other units. In the deeper layers, a hidden unit will see a larger region of the image.

__Cost function__
Defined in two parts-
Part 1 : content cost
J<sub>content</sub>(C, G)
function of content image and generated image
J<sub>style</sub>(S, G)
function of style image and generated image
J(G) = J<sub>content</sub>(C, G) + J<sub>style</sub>(S, G)

Find the generated image G
1. Initiate G randomly
   G: 100 X 100 X 3
2. Use gradient descent to minimize j(G)
G = G - (d J(G) /dG)

Content cost function :-
J(G) = Œ±J<sub>content</sub>(C, G) + Œ≤J<sub>style</sub>(S, G)

Say you use hidden layer l to compute content cost. Use a pre-trained ConvNet (eg VGG Network). Let a<sup>[l] (C)</sup> and a<sup>[l] (G)</sup> be the activation of layer l on the images. If a<sup>[l] (C)</sup> and a<sup>[l] (G)</sup> are similar, both images have similar content.

J<sub>content</sub>(C, G) = (1/2)*||a<sup>[l] (C)</sup> - a<sup>[l] (G)</sup>||<sup>2</sup>

Style cost function :-
If we are using l layer's activation to measure style, then style can be explained as a correlation between activations across channels. Let's look into layer l 
![](@attachment/Clipboard_2023-09-30-12-03-40.png)
no of channels = 5
in first two channels, all n<sub>H</sub> X n<sub>W</sub> positions are coorelated

Style matrix

![](@attachment/Clipboard_2023-09-30-13-43-55.png)

 We have nc channels and so you have an nc by nc dimensional matrix in order to measure how correlated each pair of them is. So particular G, l, k, k prime will measure how correlated are the activations in channel k compared to the activations in channel k prime. Here, k and k prime will range from 1 through nc, the number of channels they're all up in that layer. 
![](@attachment/Clipboard_2023-09-30-14-37-02.png)
![](@attachment/Clipboard_2023-09-30-14-13-24.png)
![](@attachment/Clipboard_2023-09-30-14-14-34.png)
![](@attachment/Clipboard_2023-09-30-14-36-23.png)
![](@attachment/Clipboard_2023-09-30-14-15-53.png)

[14 x 14 x 14 x 1]*[5 x 5 x 5 x 1] ---> [10 x 10 x 10 x 16]   16 filters




























