{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkFwDVG4z5W6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SqZkask0KUd",
        "outputId": "30b1ae16-e236-4d91-ccbc-4da2b23d76ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "mnist=fetch_openml('mnist_784')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbBErZJX0v_T"
      },
      "outputs": [],
      "source": [
        "# print(mnist.DESCR)\n",
        "# print(mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXbBufVf1XZ2"
      },
      "outputs": [],
      "source": [
        "# initializing data\n",
        "x,y=mnist.data,mnist.target\n",
        "# print(y)\n",
        "x_train_orig,x_test_orig=np.array(x[:60000]),np.array(x[60000:])\n",
        "y_train_orig,y_test_orig=np.array(y[:60000]),np.array(y[60000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfb2cesA4xO-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# testing data\n",
        "  # plt.imshow(x_train_orig[0].reshape(28,28))\n",
        "  # print(y_train_orig[0])\n",
        "  # print(x_train_orig.shape)\n",
        "# number of images\n",
        "m=x_train_orig.shape[0]\n",
        "#flattening data\n",
        "x_train_flatten=x_train_orig.reshape(x_train_orig.shape[0],-1)\n",
        "x_test_flatten=x_test_orig.reshape(x_test_orig.shape[0],-1)\n",
        "x_train=x_train_flatten/255\n",
        "x_test=x_test_flatten/255\n",
        "y_train=y_train_orig.reshape(y_train_orig.shape[0],-1)\n",
        "y_test=y_test_orig.reshape(y_test_orig.shape[0],-1)\n",
        "x_train=x_train.T\n",
        "y_train=y_train.T\n",
        "y_train=y_train.astype(float)\n",
        "y_test=y_test.astype(float)\n",
        "y_temp=np.zeros((10,y.shape[0]))\n",
        "# print(y[0])\n",
        "for i in range(100):\n",
        "  for j in range(10):\n",
        "    if(j==int(y[i])):\n",
        "      # print(str(y[i])+str(j))\n",
        "      y_temp[j,i]=1\n",
        "# print(y_temp.shape)\n",
        "print(y_temp[4][0])\n",
        "y_train,y_test=y_temp[:,:60000],y_temp[:,60000:]\n",
        "print(y_train.shape)\n",
        "dim_layers=[]\n",
        "dim_layers.append(x_train.shape[0])\n",
        "print('!last layer must have 10 nodes!')\n",
        "layers_len=int(input(\"No.of hidden layers: \"))\n",
        "for i in range (layers_len):\n",
        "  n=input(\"No. of nodes in layer \"+str(i+1)+\": \")\n",
        "  dim_layers.append(int(n))\n",
        "layers_len+=1\n",
        "print(dim_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaBRRB9JFFQh"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "  A=1/(1+np.exp(-Z))\n",
        "  return A\n",
        "def relu(Z):\n",
        "  return np.where(Z>0,Z,0.01*Z)\n",
        "def softmax(Z):\n",
        "  # print('hello')\n",
        "  A=np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True)\n",
        "  return A\n",
        "def deriv_relu(Z):\n",
        "  # print(type(Z),'hi')\n",
        "  return np.where(Z>0,1,0.01)\n",
        "def deriv_softmax(Z):\n",
        "  # print('hi')\n",
        "  length=10\n",
        "  dZ=np.zeros((60000,10))\n",
        "  Z=np.transpose(Z)\n",
        "  for row in range (0,60000):\n",
        "          den=(np.sum(np.exp(Z[row,:])))*(np.sum(np.exp(Z[row,:])))\n",
        "          for col in range (0,10):\n",
        "              sums=0\n",
        "              for j in range (0,10):\n",
        "                  if (j!=col):\n",
        "                      sums=sums+(np.math.exp(Z[row,j]))\n",
        "\n",
        "              dZ[row,col]=(np.math.exp(Z[row,col])*sums)/den\n",
        "  dZ=np.transpose(dZ)\n",
        "  Z=np.transpose(Z)\n",
        "\n",
        "    # assert (dZ.shape == Z.shape)\n",
        "  return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM8l64iZ5n8M"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(dim_layers):\n",
        "    # hidden_layers contains dimensions of each hidden layer\n",
        "  parameters={}\n",
        "  for l in range(1,layers_len):\n",
        "    parameters['W'+str(l)]=np.random.randn(dim_layers[l],dim_layers[l-1])*0.001\n",
        "    parameters['b'+str(l)]=np.zeros((dim_layers[l],1))\n",
        "  return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEsHBsh-K7Ef"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev,W,b,activation):\n",
        "  # print(str(W.shape)+\"w's shape\")\n",
        "  # print(str(A_prev.shape)+\"A's shape\")\n",
        "  Z=np.dot(W,A_prev)+b\n",
        "  if activation=='sigmoid':\n",
        "    A=sigmoid(Z)\n",
        "  elif activation=='relu':\n",
        "    A=relu(Z)\n",
        "  elif activation=='softmax':\n",
        "    A=softmax(Z)\n",
        "  cache=[]\n",
        "  cache=A_prev,Z\n",
        "  return A,cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5DX2dWbgDNs"
      },
      "outputs": [],
      "source": [
        "# def one_hot(y_train):\n",
        "#     onehot_y = np.zeros((y_train.size, int(y_train.max()) + 1))\n",
        "#     onehot_y[np.arange(y_train.size), y_train] = 1\n",
        "#     onehot_y = onehot_y.T#\n",
        "# return onehot_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2eqBpHrO1Ap"
      },
      "outputs": [],
      "source": [
        "def forward_model(dim_layers,parameters,layers_len,activation):\n",
        "  caches=[]\n",
        "  A_prev=x_train\n",
        "\n",
        "  for l in range(1,layers_len-1):\n",
        "    W,b=parameters['W'+str(l)],parameters['b'+str(l)]\n",
        "    A_prev,cache=linear_activation_forward(A_prev,W,b,'relu')\n",
        "    caches.append(cache)\n",
        "\n",
        "  W,b=parameters['W'+str(layers_len-1)],parameters['b'+str(layers_len-1)]\n",
        "  AL,cache=linear_activation_forward(A_prev,W,b,'softmax')\n",
        "  caches.append(cache)\n",
        "\n",
        "  # AL=np.array(AL[:,:1])\n",
        "  # ycap=AL.argmax(axis=0)#returns index of max element\n",
        "  # print(ycap)\n",
        "  return AL,caches,parameters\n",
        "# AL,caches,parameters=forward_model(dim_layers,layers_len,'relu')\n",
        "\n",
        "def backward_model(AL,parameters,cache,activation):\n",
        "  dAL=-(np.divide(y_train,AL)-np.divide(1-y_train,AL))\n",
        "  # A,Z=zip(*caches)\n",
        "  dZ,db,dW,dA=[],[],[],[]\n",
        "  grads={}\n",
        "  dA=dAL #dA4\n",
        "\n",
        "  for l in reversed(range(1,layers_len)):#4,3,2(L=5)\n",
        "    A=cache[l-1][0] #A4 3 2\n",
        "    Z=cache[l-1][1] #Z4 3 2\n",
        "    W,b=parameters['W'+str(l)],parameters['b'+str(l)] # W4,b4\n",
        "    # print(W.shape)\n",
        "    if(l==layers_len-1):\n",
        "      dZ=dA*deriv_softmax(Z)\n",
        "    else:\n",
        "      dZ=dA*deriv_relu(Z) #Z3,Z2\n",
        "    dA=np.dot(W.T,dZ) #dA of prev layer ie layer 2 (1 based indexing)\n",
        "    db=(1/m)*np.sum(dZ,axis=0,keepdims=True)\n",
        "    dW=(1/m)*np.dot(dZ,A.T) #A2,A1\n",
        "    # print(str(dW.shape)+'bckwrd')\n",
        "    grads['dW'+str(l)]=dW\n",
        "    grads['db'+str(l)]=db\n",
        "\n",
        "  return dA,dZ,grads\n",
        "\n",
        "def cross_entropy(AL,y_train):\n",
        "  return - np.sum(np.log(AL)*(y_train),axis=1)\n",
        "\n",
        "def final_cost(AL,y_train):\n",
        "  return np.mean(cross_entropy(AL,y_train))\n",
        "\n",
        "def plot_costs(costs, learning_rate):\n",
        "  plt.plot(np.squeeze(costs))\n",
        "  plt.ylabel('cost')\n",
        "  plt.xlabel('iterations (per hundreds)')\n",
        "  plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj80IqC6CbE1"
      },
      "outputs": [],
      "source": [
        "def update_params(parameters,grads,learning_rate):\n",
        "  for l in range(1,layers_len): #123\n",
        "    dW=grads['dW'+str(l)]\n",
        "    db=grads['db'+str(l)]\n",
        "    # print(dW.shape)\n",
        "    parameters['W'+str(l)]=parameters['W'+str(l)]-(learning_rate*dW)\n",
        "    parameters['b'+str(l)]=parameters['b'+str(l)]-(learning_rate*db)\n",
        "    # print(dW,db)\n",
        "  return parameters\n",
        "# parameters=update_params(parameters,grads,learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vfBQM5g1x_5"
      },
      "outputs": [],
      "source": [
        "#prolly final block\n",
        "# num_iterations=int(input(\"Enter number of iterations\"))\n",
        "# learning_rate=float(input(\"Enter magnitude of learning rate\"))\n",
        "num_iterations=300\n",
        "learning_rate=0.01\n",
        "costs=[]\n",
        "parameters={}\n",
        "parameters=initialize_parameters(dim_layers)\n",
        "for i in range (0,num_iterations):\n",
        "  AL,caches,parameters=forward_model(dim_layers,parameters,layers_len,'relu')\n",
        "  dA,dZ,grads=backward_model(AL,parameters,caches,'relu')\n",
        "  parameters=update_params(parameters,grads,learning_rate)\n",
        "  cost=final_cost(AL,y_train)\n",
        "  if i % 10 == 0 or i == num_iterations - 1:\n",
        "    print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "  if i % 10 == 0 or i == num_iterations:\n",
        "    costs.append(cost)\n",
        "plot_costs(costs, learning_rate=0.01)\n",
        "print(AL[5][0])\n",
        "print(AL.argmax(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVM3sv68q6B5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}