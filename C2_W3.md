---
title: C2_W3
created: '2023-09-13T13:21:28.640Z'
modified: '2023-09-13T14:06:30.976Z'
---

# C2_W3

__Hyperparameter Tuning__

A hyperparameter simply refers to a configuration of a machine learning model that has to be set before the training begins. Hyperparameters cannot be inferred by the model, but are more like external specifications that properly define how a machine learning model should run.
Examples : 
learning rate of a neural network, the number of trees in a random forest algorithm, the depth of a decision tree, and so on.

Hyperparameters in Simple Neural Networks:
1. Learning Rate : 
Specifies how fast a neural network updates its gradient parameters. If the learning rate is too small, the model will converge or descend slowly and this can be computationally expensive. However, if the learning rate is too large, the model may take gigantic descents and miss the global minimum. A good practice is to use a decaying learning rate
2. Number of iterations for training
Number of times your model performs forward and backward propagations, as well as updates its weight and bias parameters.
3. Number of Neurons in each layer
A small number of neurons could cause underfitting, while a large number of hidden layers could cause overfitting. Often times, using the same number of units in each hidden layer works well. 
4. Activation Functions
functions that learn non-linear features in every layer of a neural network. This activation functions helps the network to learn the non-linear properties of the data set. Rectified Linear Unit is usually the most popular choice for hidden layers.
5. Weight Initialization
 weights can affect how quickly or if at all the local minimum is found by the network training algorithm. This problem is often known as the vanishing or exploding gradient problem. Usually, random numbers from a normal or Gaussian distribution are advised, with a very small standard deviation (about 0.1).

___

__Batch Normalization__
Batch normalization is an algorithm to overcome the problem of internal covariance shift in a deep neural network with mini-batches. 
Normalization is a preprocessing step to standardize the data before it is sent for training in a neural network. In other words, normalization is applied to put all data points at the same scale.
Mini-batches — These are sub-batches of a training data batch that are fed to a neural network for training.

Change in the distribution of inputs to the deep layers in a neural network is called covariance shift. Change in the distribution is caused when mini-batches are applied in a neural network and each mini-batch gets a random weight and bias assigned. 

To apply normalization,  it is important to the calculate mean (m) and standard deviation (s) of each input variable (x) to a layer per mini-batch. Normalization (x̂) is applied further using the formula below.

x̂ = (x — m) / s

![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*JvYl8utfFFM-PylYlhkf7w.png)

___

__Batch Normalization as regularization__

Each mini-batch is scaled by the mean/variance computed on just that mini-batch.
This adds some noise to the values z<sup>[i]</sup> within that mini-batch. So similar to dropout, it adds some noise to each hidden layer's activations.
This has a slight regularization effect

___

__Softmax regression__
























