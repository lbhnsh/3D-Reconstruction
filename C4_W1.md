---
attachments: [Clipboard_2023-09-21-16-42-28.png, Clipboard_2023-09-21-16-43-23.png, Clipboard_2023-09-21-16-43-43.png, Clipboard_2023-09-21-16-44-21.png, Clipboard_2023-09-21-23-41-25.png, Clipboard_2023-09-26-17-47-34.png, Clipboard_2023-09-26-17-49-12.png]
title: C4_W1
created: '2023-09-21T11:11:01.550Z'
modified: '2023-09-26T12:41:18.150Z'
---

# C4_W1

CNN stands for Convolutional Neural Network,and it is a class of deep learning models primarily used for image and video recognition tasks.
Although programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. A convolution layer transforms an input volume into an output volume of different size, as shown below.

![](@attachment/Clipboard_2023-09-21-16-42-28.png)

The key idea behind CNNs is the use of convolutional layers, which apply filters or kernels to input images to detect specific patterns and features. These filters slide over the input image, and at each position, they perform element-wise multiplication and summation to produce a feature map, capturing relevant patterns in the local region. This process allows the CNN to learn hierarchical representations of the input data, starting from low-level features (e.g., edges, textures) and gradually learning more complex and abstract features.

__Vertical Edge Detection__

Y = $\begin{bmatrix} 3\ 0\ 1\ 2\ 7\ 4\\ 1\ 5\ 8\ 9\ 3\ 1\\ 2\ 7\ 2\ 5\ 1\ 3\\ 0\ 1\ 3\ 1\ 7\ 8\\ 4\ 2\ 1\ 6\ 2\ 8\\ 2\ 4\ 5\ 2\ 3\ 9\end{bmatrix}$  *  $\begin{matrix} 1\ 0\ -1\\ 1\ 0\ -1\\ 1\ 0\ -1\end{bmatrix}$  = $\begin{bmatrix} -5\ -4\ 0\ 8\\ -10\ -2\ 2\ 3\\ 0\ -2\ -4\ -7\\ -3\ -2\ -3\ -16\end{bmatrix}$

[6 X 6] matrix  *  filter kernel  =  [4 X 4] matrix

For Horizontal edge detection, filter kernel : $\begin{bmatrix} 1\ 1\ 1\\ 0\ 0\ 0\\ -1\ -1\ -1\end{bmatrix}$

![](@attachment/Clipboard_2023-09-21-16-43-23.png)

![](@attachment/Clipboard_2023-09-21-16-43-43.png)

Usually in square matrices, the elements along edge are processed only once while those in central portion are processesd repeatedly. I deeper networks, this leads to the problem of overfitting. Tp overcome this, we implement the concept of Padding in which we add one or two additional rows and columns each with null ('0') value so that edge element are not left out during processing

The main benefits of padding are:

It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the "same" convolution, in which the height/width is exactly preserved after one layer.

It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels at the edges of an image.

![](@attachment/Clipboard_2023-09-21-16-44-21.png)

![](@attachment/Clipboard_2023-09-21-23-41-25.png)

Convolutions without use of padding o]are called 'Valid' whereas those using convolutions are called 'Same'.
In same convolutions padding is also done so that output size is the same as input size of matrix

__Strided Convolutions__

n X n  with padding p  *  f X f  with stride s = [(n + 2p - f)/s + 1] * [(n + 2p - f)/s + 1]   

__Convolutions over volume (RGB) images__

A = height * width * channels = 6 X 6 X 3
F = 3 X 3 X 3
B = 4 X 4
A * F = B

Multiple filters
A = height * width * channels = 6 X 6 X 3
F1 (vertical edge) = 3 X 3 X 3
F2 (horizontal edge) = 3 X 3 X 3
B1 = 4 X 4
B2 = 4 X 4
B = 4 X 4 X 2

Summary : [n X n X n<sub>c</sub>]*[f X f X n<sub>c</sub>] = [(n-f+1) X (n-f+1) X n<sub>c</sub>']  
where n<sub>c</sub>' is the number of filters

__Example of a Layer__

A = [6 X 6 X 3]
F1 = [3 X 3 X 3]
F1 = [3 X 3 X 3]
B1 = [4 X 4] + b<sub>1</sub>
B2 = [4 X 4] + b<sub>2</sub>

A * $\begin{bmatrix} F1\\F2\end{bmatrix}$ ---->  $\begin{bmatrix} B1\\B2\end{bmatrix}$  = [4 X 4 X 2]

No of parameters in one layer

If you have 10 filters that are 3 X 3 X 3 in one layer of a neural network, how many parameters does that layer have ?

3X3X3 = 27 parameters
+ bias ---->  28 parameters
For 10 filters, 280 parameters

__Summary of Notation__
If layer l is a convolution layer:
f<sup>[l]</sup> = filter size
p<sup>[l]</sup> = padding
s<sup>[l]</sup> = stride
n<sub>c</sub><sup>[l]</sup> = number of filters
Each filter is : f<sup>[l]</sup> X f<sup>[l]</sup> X n<sub>c</sub><sup>[l-1]</sup>
Activations : a<sup>[l]</sup> = n<sub>H</sub><sup>[l]</sup> X n<sub>w</sub><sup>[l]</sup> X n<sub>c</sub><sup>[l]</sup>

Weights : f<sup>[l]</sup> X f<sup>[l]</sup> X n<sub>c</sub><sup>[l-1]</sup> X n<sub>c</sub><sup>[l]</sup>

bias : n<sub>c</sub><sup>[l]</sup> - (1,1,1,n<sub>c</sub><sup>[l]</sup>)

Input : n<sub>H</sub><sup>[l-1]</sup> X n<sub>w</sub><sup>[l-1]</sup> X n<sub>c</sub><sup>[l-1]</sup>

Output : n<sub>H</sub><sup>[l]</sup> X n<sub>w</sub><sup>[l]</sup> X n<sub>c</sub><sup>[l]</sup>

n<sub>w</sub><sup>[l]</sup> = [((n<sub>w</sub><sup>[l-1]</sup> + 2p<sup>[l]</sup> - f<sup>[l]</sup>)/s<sup>[l]</sup>) +1]

A<sup>[l]</sup> = m X n<sub>H</sub><sup>[l]</sup> X n<sub>w</sub><sup>[l]</sup> X n<sub>c</sub><sup>[l]</sup>

__Types of layers on a Convolutional network__

Convolution

Pooling
Pooling layers reduce the spatial dimensions of the feature maps and downsample the learned representations. MaxPooling is a common pooling operation that keeps the maximum value from a local region and discards the rest.

Fully Connected
After multiple convolutional and pooling layers, the features are flattened and fed into fully connected layers, which perform classification or regression based on the learned representations.

Output layer

__Pooling layer : Max pooling__
![](@attachment/Clipboard_2023-09-26-17-47-34.png)

__Pooling layer : Average pooling__
![](@attachment/Clipboard_2023-09-26-17-49-12.png)

Hyperparameters : 
f : filter size
s : stride
Max or Average pooling

n<sub>H</sub> X n<sub>w</sub> X n<sub>c</sub>
[((n<sub>H</sub> - f)/s) + 1] X [((n<sub>H</sub> - f)/s) + 1] X n<sub>C</sub>

__Why convolutions ?__
1. Parameter sharing : 
A feature detector (such as a vertical edge detector) that's useful in one part of the image is probably useful in another part of the image.
2. Sparsity of connections : 
In each layer, each output value depends only on a small number of input






































