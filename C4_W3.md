---
attachments: [Clipboard_2023-09-28-09-36-47.png, Clipboard_2023-09-28-09-37-17.png, Clipboard_2023-09-28-09-49-08.png, Clipboard_2023-09-28-10-43-02.png, Clipboard_2023-09-28-13-16-35.png, Clipboard_2023-09-28-13-39-28.png, Clipboard_2023-09-28-18-36-25.png, Clipboard_2023-09-28-18-37-36.png, Clipboard_2023-09-28-18-39-17.png, Clipboard_2023-09-28-20-33-11.png, Clipboard_2023-09-28-20-33-59.png, Clipboard_2023-09-28-20-40-21.png, Clipboard_2023-09-28-21-43-05.png, Clipboard_2023-09-28-21-47-00.png, Clipboard_2023-09-28-21-54-14.png, Clipboard_2023-09-29-00-10-14.png, Clipboard_2023-09-29-00-16-49.png, Clipboard_2023-09-29-00-21-46.png]
title: C4_W3
created: '2023-09-28T03:48:57.301Z'
modified: '2023-09-28T18:51:48.401Z'
---

# C4_W3
__What are localization and detection ?__
Algorithm not only has to identify the picture as car but also detect position of car and draw a red colored rectangle around it. Detect car as well as its position.

Defining the target label y
1. pedestrian
2. car
3. motorcycle
4. background

need to output b<sub>x</sub>, b<sub>y</sub>, b<sub>h</sub>, b<sub>w</sub>, class label (1-4)

y = $\begin{bmatrix}Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\end{bmatrix}$

![](@attachment/Clipboard_2023-09-28-09-36-47.png) 
y = $\begin{bmatrix}1\\bx\\by\\bh\\bw\\0\\1\\0\end{bmatrix}$
![](@attachment/Clipboard_2023-09-28-09-37-17.png)
y = $\begin{bmatrix}0\\bx\\by\\bh\\bw\\0\\0\\0\end{bmatrix}$

L(y^, y) = {(y^<sub>1</sub> - y<sub>1</sub>)<sup>2</sup>, (y^<sub>2</sub> - y<sub>2</sub>)<sup>2</sup>, (y^<sub>3</sub> - y<sub>3</sub>)<sup>2</sup>, ...}

output only x,y co-ordinates of important points called landmarks. 
![](@attachment/Clipboard_2023-09-28-09-49-08.png)
output:
l<sub>1x</sub>,l<sub>1y</sub> l<sub>2x</sub>,l<sub>2y</sub> l<sub>3x</sub>,l<sub>3y</sub>,...

Sliding windows detection:
Take these windows, these square boxes, and slide them across the entire image and classify every square region with some stride as containing a car or not. Now there's a huge disadvantage of Sliding Windows Detection, which is the computational cost. Because of  cropping out so many different square regions in the image and running each of them independently through a ConvNet. And if you use a very coarse stride, a very big stride, a very big step size, then that will reduce the number of windows you need to pass through the ConvNet, but that courser granularity may hurt performance. Whereas if you use a very fine granularity or a very small stride, then the huge number of all these little regions you're passing through the ConvNet means that means there is a very high computational cost. However, this problem of computational cost has a pretty good solution. In particular, the Sliding Windows Object Detector can be implemented convolutionally or much more efficiently.
__Convolutional implementation of sliding windows__

Turning FC layer into convolutional layers

[14 X 14 X 3]----5 X 5---->[10 X 10 X 16]----2 X 2 (Max Pool)---->[5 X 5 X 16]---FC 5X5--->[1 X 1 X 400]---FC 1X1--->[1 X 1 X 400]---FC 1X1---> y, softmax(4) [1X1X4]

![](@attachment/Clipboard_2023-09-28-10-43-02.png)

[28 X 28]---5X5--->[16 X 16]---2X2 MAXPOOL--->[12 X 12]---5X5--->[8 X 8 X 400]---1X1--->[8 X 8 X 400]---1X1--->[8 X 8 X 4]

This algorithm has one weakness which is the position of the bounding boxes is not going to be too accurate

__YOLO algorithm for Bounding Box Predictions__
With sliding windows, we take three sets of locations and run the crossfire through it. And in this case, none of the boxes really match up perfectly with the position of the car. If any box is best match, a good way to get this output more accurate bounding boxex is with the YOLO algorithm. We divide image into 9 grids and basic idea is to apply image classification and localization algorithm to all 9 grids. Label Y for training for each grid cell is 8-dimensional vector. 
y = $\begin{bmatrix}Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\end{bmatrix}$
Target output:
3 X 3 X 8

AS long as we don't have more than one object in each grid cell, the algorithm should work okay. Multiple objects in a grid cell cause problems. 

Bounding boxes of any aspect ratio can be output as well as output much more precise co-ordinates that aren't just dictated by the stripe size of your sliding windows classifier. CNN implementation using YOLO (You Only Look Once) is very fast and works even for real time object detection.

y = $\begin{bmatrix}1\\bx\\by\\bh\\bw\\0\\1\\0\end{bmatrix}$

If value is not between 0 and 1 it was outside the square.

__Intersection Over Union__
IoU function computes the intersection over the union of these two bounding boxes
![](@attachment/Clipboard_2023-09-28-13-16-35.png)
Intersection of union computes the size of the intersection. 
![](@attachment/Clipboard_2023-09-28-13-39-28.png)
So that orange shaded area divided by size of green shaded area gives the IoU. 
Correct if IoU >= 0.50

One of the problems of object detection is that the algorithm makes multiple detection for the same object rather than detecting an object just once at a time.  

Non Max Suppression
place 19 X 19 grid over the car image so that the image has only one midpoint. Running object classification and localization for every one of these 361 split cells. Many cells will predict possibility of presence of object in the image. This leads to multiple detections of the same object. Non Max Suppression helps to make only one detection per object
Outputs the maximum probabilities classifications and suppress the non-maximal ones.

Input 19 X 19 grid
Output 19 X 19 X 8 volume
Each output prediction is $\begin{bmatrix}pc\\bx\\by\\bh\\bw\end{bmatrix}$
Discard all boxes with p<sub>c</sub> <= 0.6
While there are any remaining boxes :
Pick the box with the largest p<sub>c</sub> and output them as a prediction
Discard any remaining box with IoU >= 0.5 with the box output in the previous step

__Anchor boxes__

One of the problems with object detection. is that each of the grid cells can detect only one object. What if a grid cell wants to detect multiple objects?  The midpoint of the pedestrian and the midpoint of the car are in almost the same place and both of them fall into the same grid cell. So, for that grid cell, if Y outputs this vector where you are detecting three causes, pedestrians, cars and motorcycles, it won't be able to output two detections. So I have to pick one of the two detections to output. 

![](@attachment/Clipboard_2023-09-28-18-36-25.png)

y = $\begin{bmatrix}Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\end{bmatrix}$

![](@attachment/Clipboard_2023-09-28-18-37-36.png)

y = ![](@attachment/Clipboard_2023-09-28-18-39-17.png)

Previously each object in training image is assigned to grid cell that contains that object's midpoint. Output y = 3 X 3 X 8

With two anchor boxes each object in training image is assigned to grid cell that contains object's midpoint and anchor box for the grid cell with highest IoU

y = $\begin{bmatrix}Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\\Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\end{bmatrix}$

__YOLO Implementation__
Suppose you're trying to train an algorithm to detect three objects: pedestrians, cars, and motorcycles. Additionally we also need one background class. 
y is 3 X 3 X 8
1 = pedestrian
2 = car
3 = motorcycle
y = $\begin{bmatrix}Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\\Pc\\bx\\by\\bh\\bw\\c1\\c2\\c3\end{bmatrix}$

![](@attachment/Clipboard_2023-09-28-20-33-59.png)

For each grid cell, get 2 predicted bounding boxes. 
Get rid of low probability predictions
For each class use Non-Max suppression to generate final predictions

Most effective CV algorithm


__Semantic Segmentation with U-Net__
Goal is to draw a careful outline around the object that is detected so that you know exactly which pixels belong to object and which pixels don't.
![](@attachment/Clipboard_2023-09-28-20-40-21.png)
With semantic segmentation, algorithm aims to label every pixel. 

Per-pixel class labels
![](@attachment/Clipboard_2023-09-28-21-43-05.png)

![](@attachment/Clipboard_2023-09-28-21-47-00.png)

As we go deeper into the network, height and width will go back up while the number of channels will decrease. It does not contain last two FC layers. 
output segmentation map ![](@attachment/Clipboard_2023-09-28-21-54-14.png)

It works on the concept of Transpose Convolution

__Transpose Convolution__
Output is bigger than the original inputs

[2 X 2] * [3 X 3] = [4 X 4]

Instead of placing filter on input, we place input on the output

__U-Net architecture__

Deep learning for semantic segmentation

![](@attachment/Clipboard_2023-09-29-00-10-14.png)

input is an image of dimensions [h X w X 3]
followed by three convolutional layers
whose dimensions change as follows
![](@attachment/Clipboard_2023-09-29-00-16-49.png)
This is first half of the architecture
after this we apply Transpose convolutions

![](@attachment/Clipboard_2023-09-29-00-21-46.png)





















